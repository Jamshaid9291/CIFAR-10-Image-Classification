{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 3649,
          "databundleVersionId": 46718,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30301,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jamshaid9291/CIFAR-10-Image-Classification/blob/main/cifar10_custom_resnet_cnn_pytorch_97_acc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you like my notebook, I would really appreciate an upvote !**"
      ],
      "metadata": {
        "id": "6x90SH4xpoX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction:\n",
        "\n",
        "The CIFAR-10 dataset contains 60,000 color images of 32 x 32 pixels in 3 channels divided into 10 classes. Each class contains 6,000 images. The training set contains 50,000 images, while the test sets provides 10,000 images. This image taken from the CIFAR repository ( https://www.cs.toronto.edu/~kriz/cifar.html ). This is a classification problem with 10 classes(muti-label classification). We can take a view on this image for more comprehension of the dataset."
      ],
      "metadata": {
        "id": "xUCF9gJKpoX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Label Encoding is as follows:**\n",
        "\n",
        "| **Label** | **Type** |\n",
        "| -: | :-: |\n",
        "| 0 | airplane |\n",
        "| 1 | automobile |\n",
        "| 2 | bird |\n",
        "| 3 | cat |\n",
        "| 4 | deer |\n",
        "| 5 | dog |\n",
        "| 6 | frog |\n",
        "| 7 | horse |\n",
        "| 8 | ship |\n",
        "| 9 | truck |"
      ],
      "metadata": {
        "id": "ek-siDg7poYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "torch.manual_seed(1) # Set manual seed\n",
        "from sklearn import preprocessing\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "\n",
        "# Define GPU device - We will run our model on GPU\n",
        "mps_device = torch.device(\"mps\")"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T03:35:34.14164Z",
          "start_time": "2023-01-12T03:35:31.219551Z"
        },
        "id": "7xOe91VGpoYB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Preparation\n",
        "\n",
        "\n",
        "The idea here is to pass as much as different transformed images to our model so that our model learns complex patterns and restricts itself to overfit.\n",
        "\n",
        "The following steps have been implemented for the same\n",
        "\n",
        "- *Image resizing* : As the image size from source is 32X32 we apply resize function just to make sure every images passed to our process is similar in terms of size\n",
        "\n",
        "\n",
        "- *Image transformation*: We apply different transformation to the image. Details mentioned below. Here one point to note is that we can randomly apply every type of transformation as this might lead to underfitting. The reason behind this is that the resolution of the images is already very low and applying some critical transfortions like centercrop, vertical flip and other might decay model performance\n",
        "\n",
        "\n",
        "- *Apply normalization to image*: The mean and standard deviation is taken from different studies researchers have implemented and have them to converge faster. You can experiment with different image size and standard deviations as well\n",
        "\n",
        "\n",
        "- *Apply Random Erasing*: It helps in randomly adding some noise to the image, so that the model can learn to generalize better."
      ],
      "metadata": {
        "id": "_MaxLDu7poYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the image size and transformation pipeline for training and validation**"
      ],
      "metadata": {
        "id": "-38iiQhEpoYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 32\n",
        "\n",
        "mean, std = [0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261]\n",
        "# These values are mostly used by researchers as found to very useful in fast convergence\n",
        "\n",
        "\n",
        "# https://pytorch.org/vision/stable/transforms.html\n",
        "# We can try various transformation for good generalization of model\n",
        "composed_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), # Resize the image in a 32X32 shape\n",
        "                                     transforms.RandomRotation(20), # Randomly rotate some images by 20 degrees\n",
        "                                     transforms.RandomHorizontalFlip(0.1), # Randomly horizontal flip the images\n",
        "                                     transforms.ColorJitter(brightness = 0.1, # Randomly adjust color jitter of the images\n",
        "                                                            contrast = 0.1,\n",
        "                                                            saturation = 0.1),\n",
        "                                     transforms.RandomAdjustSharpness(sharpness_factor = 2,\n",
        "                                                                      p = 0.1), # Randomly adjust sharpness\n",
        "                                     transforms.ToTensor(),   # Converting image to tensor\n",
        "                                     transforms.Normalize(mean, std), # Normalizing with standard mean and standard deviation\n",
        "                                     transforms.RandomErasing(p=0.75,scale=(0.02, 0.1),value=1.0, inplace=False)])\n",
        "\n",
        "\n",
        "composed_test = transforms.Compose([transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean, std)])\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T03:35:34.154114Z",
          "start_time": "2023-01-12T03:35:34.149023Z"
        },
        "id": "nTTI1BQPpoYC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load train and validation dataset**"
      ],
      "metadata": {
        "id": "XaGTJrOfpoYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data and transform the dataset\n",
        "train_dataset =  dsets.CIFAR10(root='./data', train=True, download=True, transform = composed_train)\n",
        "validation_dataset = dsets.CIFAR10(root='./data', train=False, download=True, transform = composed_test)\n",
        "\n",
        "# Create train and validation batch for training\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T03:35:35.774839Z",
          "start_time": "2023-01-12T03:35:34.161393Z"
        },
        "id": "3xY046CppoYD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Some important functions to print the images and image convertor from tensor to numpy**"
      ],
      "metadata": {
        "id": "YdzvlbWNpoYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_data(img):\n",
        "    try:\n",
        "        plt.imshow(img[0])\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "    print(img[0].shape, img[0].permute(1,2,0).shape)\n",
        "    plt.imshow(img[0].permute(1,2,0))\n",
        "    plt.title('y = '+ str(img[1]))\n",
        "    plt.show()\n",
        "\n",
        "# We need to convert the images to numpy arrays as tensors are not compatible with matplotlib.\n",
        "def im_convert(tensor):\n",
        "    #Lets\n",
        "    img = tensor.cpu().clone().detach().numpy() #\n",
        "    img = img.transpose(1, 2, 0)\n",
        "    img = img * np.array(tuple(mean)) + np.array(tuple(std))\n",
        "    img = img.clip(0, 1) # Clipping the size to print the images later\n",
        "    return img"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T03:35:35.789356Z",
          "start_time": "2023-01-12T03:35:35.78525Z"
        },
        "id": "Lf7GmevtpoYD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lets print some random datasets**"
      ],
      "metadata": {
        "id": "3ArrD0mspoYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_data(train_dataset[8])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T03:35:35.92177Z",
          "start_time": "2023-01-12T03:35:35.797086Z"
        },
        "id": "tkl1fUJTpoYD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Different classes in CIPHAR 10 dataset.\n",
        "classes = ('airplane',\n",
        "           'automobile',\n",
        "           'bird',\n",
        "           'cat',\n",
        "           'deer',\n",
        "           'dog',\n",
        "           'frog',\n",
        "           'horse',\n",
        "           'ship',\n",
        "           'truck')\n",
        "\n",
        "# Define an iterable on the data\n",
        "data_iterable = iter(train_loader) # converting our train_dataloader to iterable so that we can iter through it.\n",
        "images, labels = next(data_iterable) #going from 1st batch of 100 images to the next batch\n",
        "fig = plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Lets plot 50 images from our train_dataset\n",
        "for idx in np.arange(10):\n",
        "    ax = fig.add_subplot(2, 5, idx+1, xticks=[], yticks=[])\n",
        "\n",
        "    # Note: imshow cant print tensor !\n",
        "    # Lets convert tensor image to numpy using im_convert function for imshow to print the image\n",
        "    plt.imshow(im_convert(images[idx]))\n",
        "    ax.set_title(classes[labels[idx].item()])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T06:46:12.464082Z",
          "start_time": "2023-01-12T06:46:12.211329Z"
        },
        "id": "y2VjkwMOpoYE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Convolution Neural Network Implementation"
      ],
      "metadata": {
        "id": "at2hYJS2poYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps followed for the modelling process:\n",
        "\n",
        "1) Define a rough model architecture to follow (credits to IBM for image)\n",
        "\n",
        "2) Starting with 2 Convolution layer and 1 fully connected layer (fc)\n",
        "\n",
        "3) Keep adding hidden layers to the above model and check of validation accuracy improves.\n",
        "\n",
        "4) Note if we increase model layers and neurons this might lead to slower training and increase chances of overfitting. Also increase complexitu might led to getting stuck at saddle point or local minima. Hence to cater all this potential problems, experimented with momentum, dropout & He weight initialization"
      ],
      "metadata": {
        "id": "Be1SkDvopoYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**i) Define common model training module**"
      ],
      "metadata": {
        "id": "CWBKU52RpoYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, validation_loader, optimizer, n_epochs = 20):\n",
        "\n",
        "    # Global variable\n",
        "    N_test = len(validation_dataset)\n",
        "    accuracy_list = []\n",
        "    train_loss_list = []\n",
        "    model = model.to(mps_device)\n",
        "    train_cost_list = []\n",
        "    val_cost_list = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        train_COST = 0\n",
        "        for x,y in train_loader:\n",
        "            x = x.to(mps_device)\n",
        "            y = y.to(mps_device)\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            z = model(x)\n",
        "            loss = criterion(z,y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_COST+=loss.item()\n",
        "\n",
        "        train_COST = train_COST/len(train_loader)\n",
        "        train_cost_list.append(train_COST)\n",
        "        correct = 0\n",
        "\n",
        "        # Perform the prediction on the validation data\n",
        "        val_COST = 0\n",
        "        for x_test, y_test in validation_loader:\n",
        "            model.eval()\n",
        "            x_test = x_test.to(mps_device)\n",
        "            y_test = y_test.to(mps_device)\n",
        "            z = model(x_test)\n",
        "            val_loss = criterion(z, y_test)\n",
        "            _, yhat = torch.max(z.data, 1)\n",
        "            correct += (yhat==y_test).sum().item()\n",
        "            val_COST+=val_loss.item()\n",
        "\n",
        "        val_COST = val_COST/ len(validation_loader)\n",
        "        val_cost_list.append(val_COST)\n",
        "\n",
        "        accuracy = correct / N_test\n",
        "        accuracy_list.append(accuracy)\n",
        "\n",
        "        print(\"--> Epoch Number : {}\".format(epoch + 1),\n",
        "              \" | Training Loss : {}\".format(round(train_COST,4)),\n",
        "              \" | Validation Loss : {}\".format(round(val_COST,4)),\n",
        "              \" | Validation Accuracy : {}%\".format(round(accuracy * 100, 2)))\n",
        "\n",
        "    return accuracy_list, train_cost_list, val_cost_list"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T03:35:38.3747Z",
          "start_time": "2023-01-12T03:35:38.355133Z"
        },
        "id": "LPNHteOhpoYE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ii) CNN Model V1:**"
      ],
      "metadata": {
        "id": "Vr6ien6wpoYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ],
      "metadata": {
        "id": "KFEINZfCpoYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Credits : IBM"
      ],
      "metadata": {
        "id": "FBFTKa2DpoYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    '''\n",
        "    CNN Model V1:\n",
        "    1. 2 convolution + max pool layers\n",
        "    2. 1 fully connected layers\n",
        "    3. Default runtime using 0 momentum and 0 dropout value\n",
        "    '''\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self, out_1 = 32, out_2 = 64, number_of_classes = 10):\n",
        "        super(CNN, self).__init__()\n",
        "        self.cnn1 = nn.Conv2d(in_channels = 3, out_channels = out_1, kernel_size = 5, padding = 2)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "\n",
        "        self.cnn2 = nn.Conv2d(in_channels = out_1, out_channels = out_2, kernel_size = 5, padding = 2)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "        self.fc1 = nn.Linear(out_2 * 8 * 8, number_of_classes)\n",
        "        # Calculation of how we got 8*8 is mentioned in the below comment\n",
        "\n",
        "    # Prediction\n",
        "    def forward(self, x):\n",
        "        x = self.cnn1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.cnn2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        return(x)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T20:51:45.55699Z",
          "start_time": "2023-01-05T20:51:45.53852Z"
        },
        "id": "O0i_vu8ypoYE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how we get the 8x8 image size after cnn1, maxpool1, cnn2 and maxpool2:\n",
        "\n",
        "   1. 'out_2' is a given parameter when  the cnn2 was defined, it is the output of cnn2 equal to 32\n",
        "\n",
        "   2. 8x8 is the result of the 'image size' after cnn1, maxpool1, cnn2 and maxpool2:\n",
        "\n",
        "2.a. Input size is 32x32\n",
        "\n",
        "2.b. after cnn1, size per channel (total of 16 channels) is (32 + 2*2 -5)/1 + 1 = 32x32\n",
        "\n",
        "2.c after maxpool 1, size per channel (still total of 32 channels) is (32 + 2*0 - 2)/2 + 1 (stride size is equal to kernel size of 2) = 16x16\n",
        "\n",
        "2.c after cnn2, size per channel is (16+ 2*2 -5)/1 + 1 =16x16\n",
        "\n",
        "2.d. after maxpool2, size per channel is (16  + 2*0 -2)/2 + 1= 8x8"
      ],
      "metadata": {
        "id": "Pvq5R54gpoYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define loss function, optimizer, dataset loader**"
      ],
      "metadata": {
        "id": "3neFFF57poYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = CNN(out_1=32, out_2=64, number_of_classes = 10)\n",
        "\n",
        "# Define model training hyperparameters\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "# Train the model\n",
        "accuracy_list_normal, train_cost_list, val_cost_list = train_model(model=model,n_epochs=20,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-05T20:51:25.37358Z",
          "start_time": "2023-01-05T20:51:19.471493Z"
        },
        "id": "xtOqfNrvpoYF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above with momentum 0 gave less accurate results hence sticking back to momentum = 0.2"
      ],
      "metadata": {
        "id": "pzU5WkE9poYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**iii) CNN Model V1 with momentum:**"
      ],
      "metadata": {
        "id": "rPk8cRU6poYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model_mmt = CNN(out_1=32, out_2=64, number_of_classes = 10)\n",
        "\n",
        "# Define the model learning hyperparameters\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model_mmt.parameters(), lr = learning_rate, momentum = 0.2)\n",
        "\n",
        "# Train the model\n",
        "accuracy_list_normal_mmt, train_cost_list_mmt, val_cost_list_mmt=train_model(model=model_mmt,n_epochs=20,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-03T16:57:27.523485Z",
          "start_time": "2023-01-03T16:48:57.592468Z"
        },
        "_kg_hide-output": true,
        "id": "uQY4dSlkpoYF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model:\n",
        "pickle.dump(model, open('./model/CNN_momentum_V1.pkl', 'wb'))\n",
        "\n",
        "#CNN_momentum_V1 - 2 CNN Layers, Momentum = 0.2,\n",
        "# model_mmt = pickle.load(open('./model/CNN_momentum_V1.pkl', 'rb'))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-03T17:02:35.023674Z",
          "start_time": "2023-01-03T17:02:35.014772Z"
        },
        "id": "LS4RJK-7poYF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**iv) CNN Model V2:**\n",
        "\n",
        "Added one more hidden layer in the Fully connected segment of the model.\n",
        "Used momentum = 0.2 and dropout value = 0.5"
      ],
      "metadata": {
        "id": "M6SWfJlepoYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_V2(nn.Module):\n",
        "    '''\n",
        "    CNN Model V2:\n",
        "    1. 2 convolution & max pool layers\n",
        "    2. 2 fully connected layers\n",
        "    3. Default runtime using 0.2 momentum and dropout value p = 0.5\n",
        "    '''\n",
        "    # Constructor\n",
        "    def __init__(self, out_1 = 32, out_2 = 64, number_of_classes = 10, p = 0):\n",
        "        super(CNN_V2, self).__init__()\n",
        "        self.cnn1 = nn.Conv2d(in_channels = 3, out_channels = out_1, kernel_size = 5, padding = 2)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "\n",
        "        self.cnn2 = nn.Conv2d(in_channels = out_1, out_channels = out_2, kernel_size = 5, padding = 2)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "        self.fc1 = nn.Linear(out_2 * 8 * 8, 1000) # Roughly taken seein the input and the output\n",
        "        self.drop = nn.Dropout(p=p)\n",
        "        self.fc2 = nn.Linear(1000, number_of_classes)\n",
        "\n",
        "    # Prediction\n",
        "    def forward(self, x):\n",
        "        x = self.cnn1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.cnn2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(self.drop(x))\n",
        "        x = self.fc2(x)\n",
        "        return(x)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-03T17:40:03.314323Z",
          "start_time": "2023-01-03T17:40:03.293794Z"
        },
        "id": "MLRBqpxupoYF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model_mmtv2 = CNN_V2(out_1=32, out_2=64, number_of_classes = 10, p=0.5)\n",
        "\n",
        "# Define model learning hyperparamters\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model_mmtv2.parameters(), lr = learning_rate, momentum = 0.2)\n",
        "\n",
        "# Train the model\n",
        "accuracy_list_normal, train_cost_list, val_cost_list=train_model(model=model_mmtv2,n_epochs=20,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-03T17:49:05.958617Z",
          "start_time": "2023-01-03T17:40:05.444982Z"
        },
        "_kg_hide-output": true,
        "id": "kI9EUiq7poYF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Takeaway*: The accuracy improved by some bits using momentum, dropout and additional fc layer combo hence we will keep momentum & dropouts intact for other models as well"
      ],
      "metadata": {
        "id": "JBlsVZr6poYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "model_mmtv2 = model_mmtv2.to(torch.device(\"cpu\"))\n",
        "pickle.dump(model_mmtv2, open('./model/CNN_momentum_V2.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "9kA8UqstpoYG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**v) CNN Model V3 :**\n",
        "\n",
        "Added one more hidden layer in the Fully connected segment of the model.\n",
        "Used momentum = 0.2 and dropout value = 0.5"
      ],
      "metadata": {
        "id": "xa6eRvbApoYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_V3(nn.Module):\n",
        "    '''\n",
        "    CNN Model V3:\n",
        "    1. 2 convolution & max pool layers\n",
        "    2. 3 fully connected layers\n",
        "    3. Default runtime using 0.2 momentum and dropout value p = 0.5\n",
        "    '''\n",
        "    # Constructor\n",
        "    def __init__(self, out_1 = 32, out_2 = 64, number_of_classes = 10, p = 0):\n",
        "        super(CNN_V3, self).__init__()\n",
        "        self.cnn1 = nn.Conv2d(in_channels = 3, out_channels = out_1, kernel_size = 5, padding = 2)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "        self.cnn2 = nn.Conv2d(in_channels = out_1, out_channels = out_2, kernel_size = 5, padding = 2)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "        # Hidden layer 1\n",
        "        self.fc1 = nn.Linear(out_2 * 8 * 8, 1000) # Roughly taken seein the input and the output\n",
        "        self.drop = nn.Dropout(p=p)\n",
        "\n",
        "        # Hidden layer 2\n",
        "        self.fc2 = nn.Linear(1000, 1000)\n",
        "\n",
        "        # Final layer\n",
        "        self.fc3 = nn.Linear(1000, 10)\n",
        "\n",
        "    # Predictiona\n",
        "    def forward(self, x):\n",
        "        x = self.cnn1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.cnn2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(self.drop(x))\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(self.drop(x))\n",
        "        x = self.fc3(x)\n",
        "        return(x)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-03T19:09:15.507392Z",
          "start_time": "2023-01-03T19:09:15.488167Z"
        },
        "id": "buW7WteDpoYG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model_mmtv3 = CNN_V3(out_1=32, out_2=64, number_of_classes = 10, p=0.5)\n",
        "\n",
        "# Define the model training hyperparameters\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model_mmtv3.parameters(), lr = learning_rate, momentum = 0.2)\n",
        "\n",
        "# Train the model\n",
        "accuracy_list_normal, loss_list_normal=train_model(model=model_mmtv3,n_epochs=20,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-03T19:18:26.296322Z",
          "start_time": "2023-01-03T19:09:17.656088Z"
        },
        "id": "SbtmtbC7poYG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Takeaway*: The accuracy improved by some perctange on further addition of the hidden layer. Lets test some more models"
      ],
      "metadata": {
        "id": "86BHCR6qpoYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "model_mmtv3 = model_mmtv3.to(torch.device(\"cpu\"))\n",
        "pickle.dump(model_mmtv3, open('./model/CNN_momentum_V3.pkl', 'wb'))\n",
        "#CNN_mom/entum_V3 - 2 CNN Layers, 2 hidden layers, Momentum = 0.2"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-03T19:19:45.882814Z",
          "start_time": "2023-01-03T19:19:45.812243Z"
        },
        "id": "U6VDIGMGpoYG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**vi) CNN Model V3_V2 :**\n",
        "\n",
        "Adding one more hidden layer & one more convolution layer"
      ],
      "metadata": {
        "id": "jcqIrfvtpoYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_V3_V2(nn.Module):\n",
        "\n",
        "    '''\n",
        "    CNN Model V3-V2:\n",
        "    1. 3 convolution & max pool layers\n",
        "    2. 3 fully connected layers\n",
        "    3. Default runtime using 0.2 momentum and dropout value p = 0.5\n",
        "    '''\n",
        "    # Constructor\n",
        "    def __init__(self, out_1 = 32, out_2 = 64, out_3 = 128, number_of_classes = 10, p = 0):\n",
        "        super(CNN_V3_V2, self).__init__()\n",
        "        self.cnn1 = nn.Conv2d(in_channels = 3, out_channels = out_1, kernel_size = 5, padding = 2)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "        self.cnn2 = nn.Conv2d(in_channels = out_1, out_channels = out_2, kernel_size = 5, padding = 2)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "        self.cnn3 = nn.Conv2d(in_channels = out_2, out_channels = out_3, kernel_size = 5, padding = 2)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "        # Hidden layer 1\n",
        "        self.fc1 = nn.Linear(out_3 * 4 * 4, 1000)\n",
        "        # 8x8 will change to 4x4 as we added a convolution & max pool layer refer calculation comment above\n",
        "        self.drop = nn.Dropout(p=p)\n",
        "\n",
        "        # Hidden layer 2\n",
        "        self.fc2 = nn.Linear(1000, 1000)\n",
        "\n",
        "        # Final layer\n",
        "        self.fc3 = nn.Linear(1000, 10)\n",
        "\n",
        "    # Predictiona\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.cnn1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        x = self.cnn2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = self.cnn3(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        x = F.relu(self.drop(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        x = F.relu(self.drop(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return(x)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-04T02:18:35.444033Z",
          "start_time": "2023-01-04T02:18:35.42473Z"
        },
        "id": "zGAUvGZ1poYK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "model_mmtv4 = CNN_V3_V2(out_1=32, out_2=64, out_3 =128, number_of_classes = 10, p=0.5)\n",
        "\n",
        "# Define the model hyperparameters\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model_mmtv4.parameters(), lr = learning_rate, momentum = 0.2)\n",
        "\n",
        "# Train the model\n",
        "accuracy_list_normalv4, loss_list_normalv4=train_model(model=model_mmtv4, n_epochs=20, train_loader=train_loader, validation_loader=validation_loader, optimizer=optimizer)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-03T19:39:37.840168Z",
          "start_time": "2023-01-03T19:30:06.323184Z"
        },
        "_kg_hide-output": true,
        "id": "LMe4Jac0poYK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Takeaway*: The accuracy again improved by some perctange on further addition of the FC layer and the convolution and max pool layer. Now lets see if we can converge faster with similar performance using He intialization"
      ],
      "metadata": {
        "id": "lK0AMPX0poYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_mmtv4 = model_mmtv4.to(torch.device(\"cpu\"))\n",
        "pickle.dump(model_mmtv4, open('./model/CNN_momentum_V4.pkl', 'wb'))\n",
        "#CNN_momentum_V4 - 3 CNN Layers, 2 hidden layers, Momentum = 0.2"
      ],
      "metadata": {
        "id": "TiyF4foppoYK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**vii) CNN Model V3-V3 :**\n",
        "\n",
        "Adding one more fully connected layer and initializing weights using He initialization"
      ],
      "metadata": {
        "id": "70NZ1kXOpoYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_V3_V3(nn.Module):\n",
        "    '''\n",
        "    CNN Model V3-V2:\n",
        "    1. 3 convolution & max pool layers\n",
        "    2. 4 fully connected layers\n",
        "    3. Default runtime using 0.2 momentum and dropout value p = 0.5\n",
        "    4. Weights initialized using He weight initialization\n",
        "    '''\n",
        "    # Constructor\n",
        "    def __init__(self, out_1 = 32, out_2 = 64, out_3 = 128, number_of_classes = 10, p = 0):\n",
        "        super(CNN_V3_V3, self).__init__()\n",
        "        self.cnn1 = nn.Conv2d(in_channels = 3, out_channels = out_1, kernel_size = 5, padding = 2)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "        self.cnn2 = nn.Conv2d(in_channels = out_1, out_channels = out_2, kernel_size = 5, padding = 2)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "        self.cnn3 = nn.Conv2d(in_channels = out_2, out_channels = out_3, kernel_size = 5, padding = 2)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "        # Hidden layer 1\n",
        "        self.fc1 = nn.Linear(out_3 * 4 * 4, 1000)\n",
        "        torch.nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
        "        self.drop = nn.Dropout(p=p)\n",
        "\n",
        "        # Hidden layer 2\n",
        "        self.fc2 = nn.Linear(1000, 1000)\n",
        "        torch.nn.init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu') # He weight initialization\n",
        "\n",
        "        # Hidden layer 3\n",
        "        self.fc3 = nn.Linear(1000, 1000)\n",
        "        torch.nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu') # He weight initialization\n",
        "\n",
        "        # Final layer\n",
        "        self.fc4 = nn.Linear(1000, 10)\n",
        "        torch.nn.init.kaiming_uniform_(self.fc4.weight, nonlinearity='relu') # He weight initialization\n",
        "\n",
        "    # Predictiona\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.cnn1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        x = self.cnn2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = self.cnn3(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        x = F.relu(self.drop(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        x = F.relu(self.drop(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        x = F.relu(self.drop(x))\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        return(x)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-03T23:20:22.26522Z",
          "start_time": "2023-01-03T23:20:22.242576Z"
        },
        "id": "FWsLTAI8poYK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model_mmtv5 = CNN_V3_V3(out_1=32, out_2=64, out_3 =128, number_of_classes = 10, p=0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model_mmtv5.parameters(), lr = learning_rate, momentum = 0.2)\n",
        "\n",
        "# Train the model\n",
        "accuracy_list_normalv5, loss_list_normalv5=train_model(model=model_mmtv5, n_epochs=20, train_loader=train_loader, validation_loader=validation_loader, optimizer=optimizer)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-03T20:01:39.212984Z",
          "start_time": "2023-01-03T19:51:48.109994Z"
        },
        "_kg_hide-output": true,
        "id": "MoFmEvZTpoYL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Takeaway*: The accuracy reduced drastically using He weight initialization. Lets drop this approach as we couldnt find global minima through this approach. Lets use traditional random weight initialization with the above architecture"
      ],
      "metadata": {
        "id": "ho4IhPP1poYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_mmtv5 = model_mmtv5.to(torch.device(\"cpu\"))\n",
        "pickle.dump(model_mmtv5, open('./model/CNN_momentum_V5.pkl', 'wb'))\n",
        "#CNN_momentum_V5 - 3 CNN Layers, 2 hidden layers, Momentum = 0.2, He Weight initialization"
      ],
      "metadata": {
        "id": "mpOmBUtFpoYL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**viii) CNN Model V3-V4 - This resulted as champion model**\n",
        "![image-4.png](attachment:image-4.png)\n",
        "\n",
        "USed similar architecture as CNN_V3_V3. The only difference is additon of more more fc layer and removal of He weight initialization.\n",
        "\n",
        "**Below is the rough architecture which my model follows:**"
      ],
      "metadata": {
        "id": "89i-RDJtpoYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)\n",
        "\n",
        "\n",
        "Credits:\n",
        "https://developersbreach.com/convolution-neural-network-deep-learning/"
      ],
      "metadata": {
        "id": "SqaXwb5HpoYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extra additions to model apart from additional convolution layer**\n",
        "1. Lets add dropout to convolution layers with p=0.2 as high value might lead to underfit as these are the initial layers from which the model extract useful features from the image.\n",
        "2. Added Batch normalization as this will help us to converge faster"
      ],
      "metadata": {
        "id": "7mwISELKpoYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_V3_V4(nn.Module):\n",
        "    \"\"\"\n",
        "    Adding one more hidden layer & dropout value & one more convolution layer\n",
        "    Total 3 hidden layers, 3 convolution layer & Batch Normalization\n",
        "    \"\"\"\n",
        "    # Constructor\n",
        "    def __init__(self, out_1 = 32, out_2 = 64, out_3 = 128, number_of_classes = 10, p = 0):\n",
        "        super(CNN_V3_V4, self).__init__()\n",
        "        self.cnn1 = nn.Conv2d(in_channels = 3, out_channels = out_1, kernel_size = 5, padding = 2)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "        self.conv1_bn = nn.BatchNorm2d(out_1)\n",
        "        self.drop_conv = nn.Dropout(p=0.2)\n",
        "\n",
        "        self.cnn2 = nn.Conv2d(in_channels = out_1, out_channels = out_2, kernel_size = 5, padding = 2)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "        self.conv2_bn = nn.BatchNorm2d(out_2)\n",
        "\n",
        "        self.cnn3 = nn.Conv2d(in_channels = out_2, out_channels = out_3, kernel_size = 5, padding = 2)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size = 2)\n",
        "        self.conv3_bn = nn.BatchNorm2d(out_3)\n",
        "\n",
        "        # Hidden layer 1\n",
        "        self.fc1 = nn.Linear(out_3 * 4 * 4, 1000)\n",
        "        self.drop = nn.Dropout(p=p)\n",
        "        self.fc1_bn = nn.BatchNorm1d(1000)\n",
        "\n",
        "        # Hidden layer 2\n",
        "        self.fc2 = nn.Linear(1000, 1000)\n",
        "        self.fc2_bn = nn.BatchNorm1d(1000)\n",
        "\n",
        "        # Hidden layer 3\n",
        "        self.fc3 = nn.Linear(1000, 1000)\n",
        "        self.fc3_bn = nn.BatchNorm1d(1000)\n",
        "\n",
        "        # Hidden layer 4\n",
        "        self.fc4 = nn.Linear(1000, 1000)\n",
        "        self.fc4_bn = nn.BatchNorm1d(1000)\n",
        "\n",
        "        # Final layer\n",
        "        self.fc5 = nn.Linear(1000, 10)\n",
        "        self.fc5_bn = nn.BatchNorm1d(10)\n",
        "\n",
        "    # Predictiona\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.cnn1(x)\n",
        "        x = self.conv1_bn(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.drop_conv(x)\n",
        "\n",
        "        x = self.cnn2(x)\n",
        "        x = self.conv2_bn(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.drop_conv(x)\n",
        "\n",
        "        x = self.cnn3(x)\n",
        "        x = self.conv3_bn(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool3(x)\n",
        "        x = self.drop_conv(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc1_bn(x)\n",
        "\n",
        "        x = F.relu(self.drop(x))\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc2_bn(x)\n",
        "\n",
        "        x = F.relu(self.drop(x))\n",
        "        x = self.fc3(x)\n",
        "        x = self.fc3_bn(x)\n",
        "\n",
        "        x = F.relu(self.drop(x))\n",
        "        x = self.fc4(x)\n",
        "        x = self.fc4_bn(x)\n",
        "\n",
        "        x = F.relu(self.drop(x))\n",
        "        x = self.fc5(x)\n",
        "        x = self.fc5_bn(x)\n",
        "\n",
        "        return(x)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T03:50:32.798589Z",
          "start_time": "2023-01-12T03:50:32.775876Z"
        },
        "id": "KtZrAUAApoYL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_mmtv5 = CNN_V3_V4(out_1=32, out_2=64, out_3 =128, number_of_classes = 10, p=0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model_mmtv5.parameters(), lr = learning_rate, momentum = 0.2)\n",
        "\n",
        "# Train the model\n",
        "accuracy_list_normalv5, train_cost_listv5, val_cost_listv5=train_model(model=model_mmtv5, n_epochs=200, train_loader=train_loader, validation_loader=validation_loader, optimizer=optimizer)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T05:58:45.697658Z",
          "start_time": "2023-01-12T03:50:33.230875Z"
        },
        "_kg_hide-output": true,
        "id": "DOhcfSgGpoYM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model_mmtv5 = model_mmtv5.to(torch.device(\"cpu\"))\n",
        "pickle.dump(model_mmtv5, open('./model/CNN_momentum_V8_12_01_23.pkl', 'wb'))\n",
        "#CNN_momentum_V5 - 3 CNN Layers, 3 hidden layers, Momentum = 0.2\n",
        "\n",
        "# model_mmtv5 = pd.read_pickle('./model/CNN_momentum_V7.pkl')\n",
        "# model_mmtv5 = model_mmtv5.to(mps_device)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T06:42:21.981961Z",
          "start_time": "2023-01-12T06:42:21.773265Z"
        },
        "id": "OWwMFcbMpoYM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model Performance Report & Error Analysis"
      ],
      "metadata": {
        "id": "dBH5fgSGpoYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**i) Printing model training loss vs accuracy:**"
      ],
      "metadata": {
        "id": "VuT7wQ38poYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots()\n",
        "color = 'tab:red'\n",
        "ax1.plot(train_cost_listv5,color=color)\n",
        "ax1.set_xlabel('epoch',color=color)\n",
        "ax1.set_ylabel('total loss',color=color)\n",
        "ax1.tick_params(axis='y', color=color)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "color = 'tab:blue'\n",
        "ax2.set_ylabel('accuracy', color=color)\n",
        "ax2.plot( accuracy_list_normalv5, color=color)\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T06:01:27.104742Z",
          "start_time": "2023-01-12T06:01:26.903829Z"
        },
        "id": "tf9YDpXQpoYM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Takeaway**\n",
        "\n",
        "As we can see that there is no sort of anomolous behavior in model traning and model converges to minima smoothly"
      ],
      "metadata": {
        "id": "UPlmBixCpoYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ii) Printing training and validation loss:**"
      ],
      "metadata": {
        "id": "HY6Z1IbspoYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_cost_listv5, 'r', label='Training Loss')\n",
        "plt.plot(val_cost_listv5,  'g',  label='Validation Loss')\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T06:01:30.115149Z",
          "start_time": "2023-01-12T06:01:30.001415Z"
        },
        "id": "k-Jt-_oppoYM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Takeaways :**\n",
        "\n",
        "We can see that the training loss and validation loss are close to each other. This gives a sense that we did a decent job in **avoiding our model to overfit.**"
      ],
      "metadata": {
        "id": "_ZLlcV4WpoYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**iii) Error Analysis: Printing Confusion Matrix & Classification Report :**"
      ],
      "metadata": {
        "id": "gLZUqHS2poYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "# iterate over test data\n",
        "for x, y in torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1):\n",
        "\n",
        "    #print('iter val', i)\n",
        "    x = x.to(mps_device)\n",
        "    y = y.to(mps_device)\n",
        "    z = model_mmtv5(x)\n",
        "    _, yhat = torch.max(z, 1)\n",
        "    pred = yhat.data.cpu().numpy()\n",
        "    y_pred.extend(pred) # Save Prediction\n",
        "\n",
        "    labels = y.data.cpu().numpy()\n",
        "    y_true.extend(labels) # Save Truth\n",
        "\n",
        "# Build confusion matrix\n",
        "cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
        "                     columns = [i for i in classes])\n",
        "plt.figure(figsize = (12,7))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.savefig('output.png')"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T06:02:30.943667Z",
          "start_time": "2023-01-12T06:01:33.045037Z"
        },
        "id": "1oodzY3hpoYN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T06:04:05.364903Z",
          "start_time": "2023-01-12T06:04:05.331539Z"
        },
        "_kg_hide-input": true,
        "id": "MUerK8JJpoYN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the mis-classified samples\n",
        "count = 0\n",
        "i = 0\n",
        "for x, y in torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1):\n",
        "    #print('iter val', i)\n",
        "    x = x.to(mps_device)\n",
        "    y = y.to(mps_device)\n",
        "    z = model_mmtv5(x)\n",
        "    _, yhat = torch.max(z, 1)\n",
        "    if yhat != y:\n",
        "        show_data(validation_dataset[i])\n",
        "        plt.show()\n",
        "        print(\"yhat: \",yhat)\n",
        "        count += 1\n",
        "    if count >= 3:\n",
        "        break\n",
        "    i+=1"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T06:04:49.989669Z",
          "start_time": "2023-01-12T06:04:49.526144Z"
        },
        "id": "CStzLCFNpoYN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Takeaway**\n",
        "\n",
        "1) Model is not able to perform well in predicting bird, cats, dogs as compared to other class.\n",
        "\n",
        "2) Overall model accuracy is decent. If we are able to get more data for cats and dogs we can improve our model performance\n",
        "\n",
        "3) As we can see in the above image model gets confused in predicting airplane an bird, which is fair enough as the underlying matrix might resemble very close to each other. Hence if we can get more training data or we can specifically add more oversampling of data for model to learn better, there is chance to further boost the accuracy."
      ],
      "metadata": {
        "id": "IXBBYVTepoYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**iv) Printing how out model performs on the first 20 images:**"
      ],
      "metadata": {
        "id": "9MlHHu0XpoYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_iterable = iter(validation_loader)\n",
        "images, labels = next(data_iterable)\n",
        "images = images.to(mps_device)\n",
        "labels = labels.to(mps_device)\n",
        "output = model_mmtv5(images)\n",
        "_, preds = torch.max(output, 1)\n",
        "\n",
        "fig = plt.figure(figsize=(25, 5))\n",
        "\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
        "    plt.imshow(im_convert(images[idx]))\n",
        "    ax.set_title(\"{} ({})\".format(str(classes[preds[idx].item()]), str(classes[labels[idx].item()])), color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T06:04:55.151415Z",
          "start_time": "2023-01-12T06:04:54.49281Z"
        },
        "_kg_hide-input": true,
        "id": "GBrHFQempoYN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Performance on Hold-Out Set"
      ],
      "metadata": {
        "id": "qPb3_-EkpoYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Randomly test our model performance on random image taken from internet and see how our model performs on totally unseen data**"
      ],
      "metadata": {
        "id": "fYHcZ4K3poYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex1\n",
        "\n",
        "url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRsa3ISspmdRq4nDC9M6pfoNh1TvukFHBzGuA&usqp=CAU'\n",
        "response = requests.get(url, stream = True)\n",
        "img = Image.open(response.raw)\n",
        "plt.imshow(img)\n",
        "\n",
        "# Image cleared to reduce size of ipynb file"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T06:05:00.130844Z",
          "start_time": "2023-01-12T06:04:59.783618Z"
        },
        "id": "DdExfRtVpoYO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert holdout image to testing format\n",
        "img = composed_test(img)\n",
        "plt.imshow(im_convert(img))\n",
        "\n",
        "image = img.to(mps_device).unsqueeze(0)\n",
        "output = model_mmtv5(image)\n",
        "_, pred = torch.max(output, 1)\n",
        "print(\"Prediction by Model:\", (classes[pred.item()]))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T06:05:02.51582Z",
          "start_time": "2023-01-12T06:05:02.423782Z"
        },
        "id": "IV_klDrOpoYO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex2\n",
        "\n",
        "url = 'https://cdn.pixabay.com/photo/2016/02/19/15/46/labrador-retriever-1210559__480.jpg'\n",
        "response = requests.get(url, stream = True)\n",
        "img = Image.open(response.raw)\n",
        "plt.imshow(img)\n",
        "\n",
        "# Image cleared to reduce size of ipynb file"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T06:05:04.362706Z",
          "start_time": "2023-01-12T06:05:04.096454Z"
        },
        "id": "riKiSOFwpoYO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert holdout image to testing format\n",
        "img = composed_test(img)\n",
        "plt.imshow(im_convert(img))\n",
        "image = img.to(mps_device).unsqueeze(0)\n",
        "output = model_mmtv5(image)\n",
        "_, pred = torch.max(output, 1)\n",
        "print(\"Prediction by Model:\", (classes[pred.item()]))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T06:05:05.999176Z",
          "start_time": "2023-01-12T06:05:05.93402Z"
        },
        "id": "MAIs8hKNpoYO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Prediction on Test"
      ],
      "metadata": {
        "id": "PDPyYXfWpoYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Define the path where images are stored by yourself.\n",
        "        # Remember, the first image is ./train/1.png\n",
        "        img_path = self.img_dir + '/' + str(self.img_labels.iloc[idx, 0]) +'.png'\n",
        "\n",
        "        # Read image. Recall how you converted an image to numpy array in the previous step.\n",
        "        image = np.asarray(Image.open(img_path).convert('RGB'))\n",
        "\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T06:05:08.731504Z",
          "start_time": "2023-01-12T06:05:08.716616Z"
        },
        "id": "w4Gx6dMEpoYO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating transform pipeline for test data\n",
        "composed_test_check = transforms.Compose([transforms.ToPILImage(),\n",
        "                                          transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.Normalize(mean, std)])\n",
        "\n",
        "SUBMISSION = os.path.join(os.getcwd(),\"data/sampleSubmission.csv\")\n",
        "test_dataset = TestImageDataset(annotations_file=SUBMISSION, img_dir='./data/test', transform=composed_test_check)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=500, shuffle=False)\n",
        "\n",
        "# Load the submission file\n",
        "submit_df = pd.read_csv(SUBMISSION)\n",
        "\n",
        "# Define an encoder\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "# Fit the encoder\n",
        "label_encoder.fit_transform(list(classes))\n",
        "\n",
        "# Iteratively predict the target and update the submission file\n",
        "model_mmtv5 = model_mmtv5.to(mps_device)\n",
        "i = 0\n",
        "y_test_pred_list = []\n",
        "for X_test, _ in torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1000):\n",
        "\n",
        "    X_test = X_test.to(mps_device) # Move X_train to the GPU\n",
        "    model_mmtv5.eval()\n",
        "\n",
        "    _, y_test_pred = torch.max(model_mmtv5(X_test), 1) # Pick the most probable label\n",
        "\n",
        "    # Move the predicted values to CPU.\n",
        "    y_test_pred = y_test_pred.cpu()\n",
        "\n",
        "    # As submission expects actual category names such as cat, the predicted values have to be transformed.\n",
        "    y_test_pred = label_encoder.inverse_transform(y_test_pred)\n",
        "    y_test_pred_list.extend(y_test_pred)\n",
        "\n",
        "# Add labels to submission file\n",
        "submit_df['label'] = y_test_pred_list"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T06:36:54.879279Z",
          "start_time": "2023-01-12T06:05:24.677915Z"
        },
        "id": "DIZZ9BxHpoYO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Save submission\n",
        "submit_df.to_csv('submission_CNNV3V4_01_2023_V2.csv', index = False)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-12T06:39:27.558688Z",
          "start_time": "2023-01-12T06:39:27.272608Z"
        },
        "id": "QKPPvI_DpoYO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ],
      "metadata": {
        "id": "Dp0T5fgIpoYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reached greater than 85% accuracy on private test data. Due to late submission it is not accepting it to be posted on private leaderboard."
      ],
      "metadata": {
        "id": "RdJuqkGrpoYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Till now we tried different customer architectures by manually experiementing around different models. We are avle to cross 85% accuracy mark. Can we do even better?\n",
        "\n",
        "The answer is yes we can: Resnet comes to rescue. Lets read about it below."
      ],
      "metadata": {
        "id": "ks-z4KtbpoYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resnet -34 The Deep Killer"
      ],
      "metadata": {
        "id": "vhrvRBPipoYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So what is Resnet:\n",
        "\n",
        "Resnet34 is a state-of-the-art image classification model, structured as a 34 layer convolutional neural network and defined in \"Deep Residual Learning for Image Recognition\". Restnet34 is pre-trained on the ImageNet dataset which contains 100,000+ images across 200 different classes. This gives a good starting point for weights initialization for our CIFAR dataset.\n",
        "\n",
        "Note this is a pre-trained model but I have manually attempted few architectures in the above code to build a model with decent accuracy.\n",
        "\n",
        "Below code is just a overview given competition spirit and accuracy war people hold ;) Kidding its just for learning."
      ],
      "metadata": {
        "id": "IhN15loGpoYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more about Restnet-34 here :\n",
        "\n",
        "https://roboflow.com/model/resnet-34#:~:text=What%20is%20Resnet34%3F,images%20across%20200%20different%20classes.\n",
        "        "
      ],
      "metadata": {
        "id": "NrX7vMVYpoYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screen Shot 2023-01-29 at 12.34.20 PM.png](attachment:75659f8f-dcdf-4059-85b5-57a4d7520e71.png)"
      ],
      "metadata": {
        "id": "KEtkkHhLpoYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Image Size & similarly get the training and validation data**"
      ],
      "metadata": {
        "id": "ga7u3dfNpoYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 224 # We need to resize the images given resnet takes input of image size >= 224\n",
        "\n",
        "mean, std = [0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261]\n",
        "# These values are mostly used by researchers as found to very useful in fast convergence\n",
        "\n",
        "\n",
        "# https://pytorch.org/vision/stable/transforms.html\n",
        "# We can try various transformation for good generalization of model\n",
        "composed_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), # Resize the image in a 32X32 shape\n",
        "                                     transforms.RandomRotation(20), # Randomly rotate some images by 20 degrees\n",
        "                                     transforms.RandomHorizontalFlip(0.1), # Randomly horizontal flip the images\n",
        "                                     transforms.ColorJitter(brightness = 0.1, # Randomly adjust color jitter of the images\n",
        "                                                            contrast = 0.1,\n",
        "                                                            saturation = 0.1),\n",
        "                                     transforms.RandomAdjustSharpness(sharpness_factor = 2,\n",
        "                                                                      p = 0.1), # Randomly adjust sharpness\n",
        "                                     transforms.ToTensor(),   # Converting image to tensor\n",
        "                                     transforms.Normalize(mean, std), # Normalizing with standard mean and standard deviation\n",
        "                                     transforms.RandomErasing(p=0.75,scale=(0.02, 0.1),value=1.0, inplace=False)])\n",
        "\n",
        "\n",
        "composed_test = transforms.Compose([transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean, std)])\n",
        "\n",
        "\n",
        "# Load the data and transform the dataset\n",
        "train_dataset =  dsets.CIFAR10(root='./data',\n",
        "                               train=True,\n",
        "                               download=True,\n",
        "                               transform = composed_train)\n",
        "validation_dataset = dsets.CIFAR10(root='./data',\n",
        "                                   train=False,\n",
        "                                   download=True,\n",
        "                                   transform = composed_test)\n",
        "\n",
        "# Create train and validation batch for training\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100)"
      ],
      "metadata": {
        "id": "Uwp0fyPepoYP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The below code was run on google colab because of GPU restrictions, I have just stiched notebooks, let me know in the comments for any doubts**"
      ],
      "metadata": {
        "id": "T41sSYYJpoYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet_34():\n",
        "    # Define the resnet model\n",
        "    resnet = torchvision.models.resnet34(pretrained=True)\n",
        "\n",
        "    # Update the fully connected layer of resnet with our current target of 10 desired outputs\n",
        "    resnet.fc = torch.nn.Linear(resnet.fc.in_features, 10)\n",
        "\n",
        "    # Initialize with xavier uniform\n",
        "    torch.nn.init.xavier_uniform_(resnet.fc.weight)\n",
        "    return resnet\n",
        "\n",
        "model_mmtv6 = resnet_34()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model_mmtv5.parameters(),\n",
        "                            lr = learning_rate,\n",
        "                            momentum = 0.2)\n",
        "\n",
        "# Define the Scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
        "\n",
        "# Train the model\n",
        "accuracy_list_normalv5, train_cost_listv5, val_cost_listv5=train_model(model=model_mmtv5,\n",
        "                                                                       n_epochs=25,\n",
        "                                                                       train_loader=train_loader,\n",
        "                                                                       validation_loader=validation_loader,\n",
        "                                                                       optimizer=optimizer,\n",
        "                                                                       scheduler = scheduler)"
      ],
      "metadata": {
        "id": "lanDKp9EpoYP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screen Shot 2023-01-29 at 12.10.30 PM.png](attachment:de313b76-bb52-461c-8484-9be3819d4b7a.png)"
      ],
      "metadata": {
        "id": "mXflsAzypoYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Result: **97.15%** isn't it dope. Thats why we say more the data better accuracy we can achieve over the same."
      ],
      "metadata": {
        "id": "pNn9GFHppoYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I hope you liked my approach. Please give upvote which help me remain motivated and add more inutitive notebook for the Data Science Community"
      ],
      "metadata": {
        "id": "Mh6jn60opoYQ"
      }
    }
  ]
}